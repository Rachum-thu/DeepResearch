# DeepResearch Bench å®Œæ•´æ‰§è¡Œæµç¨‹åˆ†æ

## ğŸ“‹ æ€»ä½“æ¶æ„

```
run_bench_subset.sh
    â””â”€â”€ run_multi_react.py
            â””â”€â”€ MultiTurnReactAgent._run()
                    â”œâ”€â”€ call_server() â†’ OpenRouter API
                    â”œâ”€â”€ TOOL_MAP[tool_name].call()
                    â”‚   â”œâ”€â”€ Search (tool_search.py)
                    â”‚   â”œâ”€â”€ Visit (tool_visit.py)
                    â”‚   â””â”€â”€ Scholar (tool_scholar.py)
                    â””â”€â”€ Output: iter1.jsonl
```

---

## ğŸ”„ è¯¦ç»†æ‰§è¡Œæµç¨‹

### é˜¶æ®µ 1ï¼šè„šæœ¬åˆå§‹åŒ– (run_bench_subset.sh)

**æ–‡ä»¶**: `script/run_bench_subset.sh`

```bash
1. åŠ è½½ç¯å¢ƒå˜é‡
   â”œâ”€â”€ source /shared/data3/.../CubeScholar/.env.keys
   â”‚   â””â”€â”€ OPENROUTER_API_KEY, JINA_API_KEYS, GEMINI_API_KEY ç­‰
   â””â”€â”€ source third_party/DeepResearch/.env
       â””â”€â”€ SERPER_KEY_ID, API_KEY, SUMMARY_MODEL_NAME ç­‰

2. é…ç½®å‚æ•°
   â”œâ”€â”€ DATASET: data/deepresearch_bench_subset/bench_queries.jsonl
   â”œâ”€â”€ OUTPUT_DIR: output/deepresearch_bench_subset
   â”œâ”€â”€ MODEL_NAME: alibaba/tongyi-deepresearch-30b-a3b
   â”œâ”€â”€ MAX_WORKERS: 1
   â””â”€â”€ ROLLOUT_COUNT: 1

3. è°ƒç”¨ Python è„šæœ¬
   â””â”€â”€ python -u inference/run_multi_react.py \
       --dataset <DATASET> \
       --output <OUTPUT_DIR> \
       --model <MODEL_NAME> \
       --max_workers <MAX_WORKERS> \
       --roll_out_count <ROLLOUT_COUNT>
```

**è¾“å…¥æ•°æ®æ ¼å¼** (`bench_queries.jsonl`):
```json
{"question": "é—®é¢˜å†…å®¹", "answer": ""}
```

---

### é˜¶æ®µ 2ï¼šä¸»æ§åˆ¶æµç¨‹ (run_multi_react.py)

**æ–‡ä»¶**: `inference/run_multi_react.py`

```python
1. æ•°æ®åŠ è½½ä¸å¤„ç†
   â”œâ”€â”€ è¯»å– JSONL æ–‡ä»¶
   â”œâ”€â”€ æ•°æ®åˆ†ç‰‡ (æ”¯æŒåˆ†å¸ƒå¼å¤„ç†)
   â””â”€â”€ æ£€æŸ¥å·²å®Œæˆçš„ä»»åŠ¡ (æ”¯æŒæ–­ç‚¹ç»­è·‘)

2. è¾“å‡ºç›®å½•ç»“æ„
   â””â”€â”€ output/deepresearch_bench_subset/
       â””â”€â”€ tongyi-deepresearch-30b-a3b_sglang/
           â””â”€â”€ bench_queries/
               â””â”€â”€ iter1.jsonl  â† æœ€ç»ˆè¾“å‡º

3. Agent åˆå§‹åŒ– (ç¬¬169-173è¡Œ)
   llm_cfg = {
       'model': 'alibaba/tongyi-deepresearch-30b-a3b',
       'generate_cfg': {
           'max_input_tokens': 320000,
           'max_retries': 10,
           'temperature': 0.6,
           'top_p': 0.95,
           'presence_penalty': 1.1
       }
   }

   test_agent = MultiTurnReactAgent(
       llm=llm_cfg,
       function_list=["search", "visit", "google_scholar"]
   )

4. å¹¶è¡Œå¤„ç† (ç¬¬177-228è¡Œ)
   â”œâ”€â”€ ThreadPoolExecutor(max_workers=1)
   â”œâ”€â”€ å¯¹æ¯ä¸ªé—®é¢˜è°ƒç”¨: test_agent._run(task, model)
   â””â”€â”€ ç»“æœå†™å…¥: iter1.jsonl
       â”œâ”€â”€ æˆåŠŸ: {"question", "answer", "messages", "prediction", "termination"}
       â””â”€â”€ å¤±è´¥: {"question", "error", "prediction": "[Failed]"}
```

---

### é˜¶æ®µ 3ï¼šReAct Agent æ¨ç†å¾ªç¯ (react_agent.py)

**æ–‡ä»¶**: `inference/react_agent.py`

#### 3.1 åˆå§‹åŒ– (_run æ–¹æ³•, ç¬¬129-146è¡Œ)

```python
è¾“å…¥:
  data = {"item": {"question": "...", "answer": ""}, "planning_port": 6001}

åˆå§‹åŒ–:
  messages = [
      {"role": "system", "content": SYSTEM_PROMPT + "Current date: 2025-11-08"},
      {"role": "user", "content": question}
  ]

  MAX_LLM_CALL_PER_RUN = 100  # æœ€å¤šè°ƒç”¨100æ¬¡
  timeout = 150åˆ†é’Ÿ
```

#### 3.2 ä¸»æ¨ç†å¾ªç¯ (ç¬¬147-235è¡Œ)

```python
while num_llm_calls_available > 0:
    round += 1

    # æ­¥éª¤1: è°ƒç”¨ LLM (OpenRouter API)
    content = self.call_server(messages, planning_port)
    # è¿”å›æ ¼å¼: "<think>...</think>\n<tool_call>{...}</tool_call>"
    #           æˆ– "<think>...</think>\n<answer>...</answer>"

    messages.append({"role": "assistant", "content": content})

    # æ­¥éª¤2: è§£æå·¥å…·è°ƒç”¨
    if '<tool_call>' in content:
        tool_call = JSON.parse(content ä¸­çš„ tool_call)
        # ç¤ºä¾‹: {"name": "search", "arguments": {"query": ["..."]}

        # æ­¥éª¤3: æ‰§è¡Œå·¥å…·
        result = self.custom_call_tool(tool_name, tool_args)

        # æ­¥éª¤4: å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯
        messages.append({
            "role": "user",
            "content": "<tool_response>\n" + result + "\n</tool_response>"
        })

    # æ­¥éª¤5: æ£€æŸ¥æ˜¯å¦å®Œæˆ
    if '<answer>' in content:
        prediction = extract_answer(content)
        break

    # æ­¥éª¤6: æ£€æŸ¥ token é™åˆ¶ (110K tokens)
    token_count = self.count_tokens(messages)
    if token_count > 110 * 1024:
        # å¼ºåˆ¶è¦æ±‚ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        messages[-1]['content'] = "You have reached max context..."
        content = self.call_server(messages)
        break
```

**å¾ªç¯ç»ˆæ­¢æ¡ä»¶**:
1. âœ… æ¨¡å‹ç”Ÿæˆ `<answer>` æ ‡ç­¾
2. â±ï¸ è¶…è¿‡150åˆ†é’Ÿ
3. ğŸ”¢ è¶…è¿‡100æ¬¡LLMè°ƒç”¨
4. ğŸ“ è¶…è¿‡110K tokens

---

### é˜¶æ®µ 4ï¼šå·¥å…·ç³»ç»Ÿ (Tools)

#### 4.1 å·¥å…·æ³¨å†Œ (ç¬¬31-38è¡Œ)

```python
TOOL_CLASS = [
    FileParser(),      # è§£æPDF/Officeæ–‡ä»¶ (å·²ç¦ç”¨)
    Scholar(),         # Googleå­¦æœ¯æœç´¢
    Visit(),          # ç½‘é¡µè®¿é—®å’Œæ€»ç»“
    Search(),         # ç½‘é¡µæœç´¢
    PythonInterpreter(), # Pythonä»£ç æ‰§è¡Œ (å·²ç¦ç”¨)
]
TOOL_MAP = {tool.name: tool for tool in TOOL_CLASS}
```

#### 4.2 Search å·¥å…· (`tool_search.py`)

```python
è¾“å…¥: {"query": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", ...]}

æµç¨‹:
1. ä½¿ç”¨ Serper API è¿›è¡Œ Google æœç´¢
   â””â”€â”€ GET https://google.serper.dev/search
       Headers: {"X-API-KEY": SERPER_KEY_ID}

2. è¿”å›æ ¼å¼:
   Search results for query "xxx":
   [1] Title: ...
       Link: https://...
       Snippet: ...
   [2] ...
```

**ç¯å¢ƒå˜é‡**: `SERPER_KEY_ID`

#### 4.3 Visit å·¥å…· (`tool_visit.py`)

```python
è¾“å…¥: {"url": ["https://...", ...], "goal": "è·å–...ä¿¡æ¯"}

æµç¨‹:
1. Jina æŠ“å–ç½‘é¡µ (jina_readpage, ç¬¬132-167è¡Œ)
   â””â”€â”€ GET https://r.jina.ai/{url}
       Headers: {"Authorization": "Bearer {JINA_API_KEYS}"}
   â””â”€â”€ è¿”å›: ç½‘é¡µçš„ markdown æ ¼å¼æ–‡æœ¬

2. LLM æ€»ç»“ç½‘é¡µ (call_server, ç¬¬99-129è¡Œ)
   â”œâ”€â”€ ä½¿ç”¨ API_KEY + API_BASE (OpenAI å…¼å®¹æ¥å£)
   â”œâ”€â”€ æ¨¡å‹: SUMMARY_MODEL_NAME (é»˜è®¤ gpt-4o-mini)
   â””â”€â”€ Prompt: EXTRACTOR_PROMPT
       è¦æ±‚è¾“å‡º JSON: {"rational", "evidence", "summary"}

3. é‡è¯•æœºåˆ¶ (ç¬¬202-221è¡Œ)
   â”œâ”€â”€ å¦‚æœæ€»ç»“å¤±è´¥ï¼Œé€æ­¥æˆªæ–­ç½‘é¡µå†…å®¹
   â”‚   â””â”€â”€ 359684 â†’ 251778 â†’ 176244 â†’ 123370 â†’ 25000 chars
   â””â”€â”€ æœ€å¤š3æ¬¡é‡è¯•

4. è¿”å›æ ¼å¼:
   The useful information in {url} for user goal {goal}:

   Evidence in page:
   [æå–çš„å…³é”®ä¿¡æ¯]

   Summary:
   [æ€»ç»“æ®µè½]
```

**ç¯å¢ƒå˜é‡**:
- `JINA_API_KEYS` - Jina Reader API
- `API_KEY` - OpenAI API Key
- `API_BASE` - OpenAI API Base URL
- `SUMMARY_MODEL_NAME` - æ€»ç»“æ¨¡å‹åç§°

#### 4.4 Scholar å·¥å…· (`tool_scholar.py`)

```python
è¾“å…¥: {"query": ["å­¦æœ¯æŸ¥è¯¢1", "å­¦æœ¯æŸ¥è¯¢2", ...]}

æµç¨‹:
1. ä½¿ç”¨ Serper API è¿›è¡Œ Google Scholar æœç´¢
   â””â”€â”€ GET https://google.serper.dev/scholar
       Headers: {"X-API-KEY": SERPER_KEY_ID}

2. è¿”å›æ ¼å¼:
   Scholar results for query "xxx":
   [1] Title: ...
       Link: https://scholar.google.com/...
       Snippet: ...
       Citations: 123
```

**ç¯å¢ƒå˜é‡**: `SERPER_KEY_ID`

---

### é˜¶æ®µ 5ï¼šLLM API è°ƒç”¨ (call_server)

**æ–‡ä»¶**: `inference/react_agent.py` (ç¬¬59-110è¡Œ)

```python
def call_server(self, msgs, planning_port, max_tries=10):
    # é…ç½®
    openai_api_key = os.getenv("OPENROUTER_API_KEY", "")
    openai_api_base = "https://openrouter.ai/api/v1"

    client = OpenAI(
        api_key=openai_api_key,
        base_url=openai_api_base,
        timeout=600.0,
    )

    # é‡è¯•æœºåˆ¶ (æŒ‡æ•°é€€é¿)
    for attempt in range(max_tries):
        try:
            chat_response = client.chat.completions.create(
                model="alibaba/tongyi-deepresearch-30b-a3b",
                messages=msgs,
                stop=["\n<tool_response>", "<tool_response>"],
                temperature=0.85,  # ä» llm_generate_cfg
                top_p=0.95,
                presence_penalty=1.1,
                max_tokens=10000
            )

            content = chat_response.choices[0].message.content

            # OpenRouter ç‰¹å®š: æå– reasoning
            reasoning_content = "<think>\n" + chat_response.choices[0].message.reasoning + "\n</think>"
            content = reasoning_content + content

            return content.strip()

        except (APIError, APIConnectionError, APITimeoutError) as e:
            # æŒ‡æ•°é€€é¿é‡è¯•
            sleep_time = base_sleep_time * (2 ** attempt) + random(0, 1)
            time.sleep(min(sleep_time, 30))

    return "vllm server error!!!"
```

**å“åº”æ ¼å¼ç¤ºä¾‹**:
```
<think>
ç”¨æˆ·è¯¢é—®...æˆ‘éœ€è¦å…ˆæœç´¢...
</think>

<tool_call>
{"name": "search", "arguments": {"query": ["æŸ¥è¯¢å…³é”®è¯"]}}
</tool_call>
```

æˆ–æœ€ç»ˆç­”æ¡ˆ:
```
<think>
æ ¹æ®æ”¶é›†çš„ä¿¡æ¯...
</think>

<answer>
[æœ€ç»ˆç ”ç©¶æŠ¥å‘Šå†…å®¹]
</answer>
```

---

### é˜¶æ®µ 6ï¼šè¾“å‡ºæ ¼å¼

**æ–‡ä»¶**: `output/deepresearch_bench_subset/tongyi-deepresearch-30b-a3b_sglang/bench_queries/iter1.jsonl`

æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡:

```json
{
  "question": "ç”¨æˆ·çš„é—®é¢˜",
  "answer": "",  // å‚è€ƒç­”æ¡ˆ(é€šå¸¸ä¸ºç©º)
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "<think>...</think>\n<tool_call>...</tool_call>"},
    {"role": "user", "content": "<tool_response>...</tool_response>"},
    ...
    {"role": "assistant", "content": "<think>...</think>\n<answer>...</answer>"}
  ],
  "prediction": "ä» <answer> æ ‡ç­¾ä¸­æå–çš„æœ€ç»ˆç­”æ¡ˆ",
  "termination": "answer" | "exceed available llm calls" | "token limit reached" | ...
}
```

**prediction å­—æ®µ**å°±æ˜¯æœ€ç»ˆç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šï¼

---

## ğŸ”‘ å…³é”®é…ç½®æ€»ç»“

### ç¯å¢ƒå˜é‡ (.env.keys + .env)

```bash
# API Keys
OPENROUTER_API_KEY     # ä¸»æ¨ç†æ¨¡å‹ (alibaba/tongyi-deepresearch-30b-a3b)
JINA_API_KEYS          # ç½‘é¡µæŠ“å– (Jina Reader)
SERPER_KEY_ID          # æœç´¢å¼•æ“ (Google/Scholar)
API_KEY                # ç½‘é¡µæ€»ç»“ (OpenAI compatible)
API_BASE               # OpenAI API Base URL
SUMMARY_MODEL_NAME     # ç½‘é¡µæ€»ç»“æ¨¡å‹ (å¦‚ gpt-4o-mini)
```

### è¶…å‚æ•°

```python
temperature: 0.85          # æ¨¡å‹åˆ›é€ æ€§
top_p: 0.95               # nucleus sampling
presence_penalty: 1.1     # é‡å¤æƒ©ç½š
max_tokens: 10000         # å•æ¬¡å“åº”æœ€å¤§token

MAX_LLM_CALL_PER_RUN: 100  # æœ€å¤šè°ƒç”¨æ¬¡æ•°
max_input_tokens: 320000   # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
timeout: 150 minutes       # è¶…æ—¶æ—¶é—´
```

### å¯ç”¨çš„å·¥å…·

```python
function_list = ["search", "visit", "google_scholar"]
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### å•ä¸ªé—®é¢˜é¢„è®¡æ¶ˆè€—

- **LLM è°ƒç”¨æ¬¡æ•°**: 10-50 æ¬¡
- **æ—¶é—´**: 5-30 åˆ†é’Ÿ
- **Tokens**:
  - è¾“å…¥: ç´¯è®¡ 50K-200K tokens
  - è¾“å‡º: ç´¯è®¡ 10K-50K tokens
- **API è°ƒç”¨**:
  - OpenRouter: 10-50 æ¬¡
  - Serper: 5-20 æ¬¡
  - Jina: 10-30 æ¬¡
  - OpenAI (æ€»ç»“): 10-30 æ¬¡

### 3ä¸ªé—®é¢˜ (subset)

- **æ€»æ—¶é—´**: 15-90 åˆ†é’Ÿ
- **å¹¶å‘æ•°**: 1 worker (ä¸²è¡Œå¤„ç†)

### 100ä¸ªé—®é¢˜ (full)

- **æ€»æ—¶é—´**: 5-10 å°æ—¶
- **å¹¶å‘æ•°**: 3 workers (å¹¶è¡Œå¤„ç†)

---

## ğŸ”§ å…³é”®ä»£ç ä½ç½®

| åŠŸèƒ½ | æ–‡ä»¶ | è¡Œå· |
|-----|------|------|
| å¯åŠ¨è„šæœ¬ | `script/run_bench_subset.sh` | å…¨éƒ¨ |
| ä¸»æ§åˆ¶æµç¨‹ | `inference/run_multi_react.py` | 13-232 |
| Agentåˆå§‹åŒ– | `inference/run_multi_react.py` | 169-173 |
| ReActå¾ªç¯ | `inference/react_agent.py` | 147-235 |
| LLMè°ƒç”¨ | `inference/react_agent.py` | 59-110 |
| å·¥å…·è°ƒç”¨ | `inference/react_agent.py` | 237-256 |
| Searchå·¥å…· | `inference/tool_search.py` | å…¨éƒ¨ |
| Visitå·¥å…· | `inference/tool_visit.py` | å…¨éƒ¨ |
| Scholarå·¥å…· | `inference/tool_scholar.py` | å…¨éƒ¨ |
| System Prompt | `inference/prompt.py` | 1-35 |

---

## ğŸ› è°ƒè¯•æŠ€å·§

### 1. æŸ¥çœ‹å®æ—¶æ—¥å¿—
```bash
tail -f output/deepresearch_bench_subset/.../iter1.jsonl
```

### 2. æ£€æŸ¥ä¸­é—´ç»“æœ
æ¯æ¬¡LLMè°ƒç”¨éƒ½ä¼šæ‰“å°:
```
Round 1: <think>...</think>\n<tool_call>...</tool_call>
```

### 3. æ£€æŸ¥Tokenä½¿ç”¨
```
round: 15, token count: 19947
```

### 4. å·¥å…·è°ƒç”¨æ—¥å¿—
```
[visit] Summary url[https://...] attempt 1/3, content length: 359684
```

---

## ğŸ“ å¾…ä¼˜åŒ–ç‚¹

1. **å¹¶å‘èƒ½åŠ›**: å½“å‰ max_workers=1ï¼Œå¯ä»¥æå‡åˆ° 3-5
2. **ç¼“å­˜æœºåˆ¶**: é‡å¤çš„æœç´¢å’Œç½‘é¡µè®¿é—®æ²¡æœ‰ç¼“å­˜
3. **é”™è¯¯æ¢å¤**: å·¥å…·å¤±è´¥æ—¶ç¼ºä¹ä¼˜é›…é™çº§
4. **æˆæœ¬ä¼˜åŒ–**: ç½‘é¡µæ€»ç»“å¯ä»¥ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
5. **è¾“å‡ºæ ¼å¼**: prediction å­—æ®µå¯èƒ½åŒ…å« `<think>` ç­‰æ ‡ç­¾ï¼Œéœ€è¦æ¸…ç†

---

ç”Ÿæˆæ—¶é—´: 2025-11-08
